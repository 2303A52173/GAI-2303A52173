{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOx3NIOU5V1yDWM/PzK9w/n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A52173/GAI-2303A52173/blob/main/GAI_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "K Sai Praneeth\n",
        "\n",
        "2303A52173\n",
        "\n",
        "batch -41\n",
        "\n",
        "Write Python code from scratch to find error metrics of deep learning model. Actual\n",
        "values and deep learning model predicted values are shown in Table 1. Also compare the results\n",
        "with the outcomes of libraries\n",
        "\n",
        "YActual YPred\n",
        "\n",
        "20      20.5\n",
        "\n",
        "30      30.3\n",
        "\n",
        "40      40.2\n",
        "\n",
        "50      50.6\n",
        "\n",
        "60      60.7\n"
      ],
      "metadata": {
        "id": "tDwXXVwiw6FC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8I7HiTuUvmtW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e721f306-399b-4747-f83f-a519c741ad6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mannual calculations :\n",
            "MSE= 0.24600000000000147\n",
            "MAE= 0.4600000000000016\n",
            "RMSE= 0.49598387070549127\n",
            "\n",
            "Using Libraries:\n",
            "Mean Squared Error (MSE): 0.24600000000000147\n",
            "Mean Absolute Error (MAE): 0.4600000000000016\n",
            "Root Mean Squared Error (RMSE): 0.49598387070549127\n",
            "\n",
            "Manual calculations match the library results!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "Y_act = np.array([20, 30, 40, 50, 60])\n",
        "Y_pre = np.array([20.5,30.3,40.2,50.6,60.7])\n",
        "S1=0\n",
        "S2=0\n",
        "for i in range(0,5):\n",
        "  S1=S1+(Y_act[i]-Y_pre[i])**2\n",
        "  S2=S2+abs(Y_act[i]-Y_pre[i])\n",
        "MSE=S1/5\n",
        "MAE=S2/5\n",
        "print(\"Mannual calculations :\")\n",
        "print(\"MSE=\",MSE)\n",
        "print(\"MAE=\",MAE)\n",
        "print(\"RMSE=\",MSE**0.5)\n",
        "#using libraries\n",
        "mse = np.mean((Y_act - Y_pre) ** 2)\n",
        "mae = np.mean(np.abs(Y_act - Y_pre))\n",
        "rmse = np.sqrt(mse)\n",
        "print(\"\\nUsing Libraries:\")\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "print(\"\\nManual calculations match the library results!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2.Write python code from scratch to find evaluation metrics of deep learning model.\n",
        "Actual values and deep learning model predicted values are shown in Table 2. Also compare the\n",
        "results with outcome of libraries\n",
        "\n",
        "YActual YP red\n",
        "\n",
        "0 0 1 1 2 0\n",
        "\n",
        "0 0 1 0 2 0\n",
        "\n",
        "0 1 1 2 2 1\n",
        "\n",
        "0 2 1 0 2 2\n",
        "\n",
        "0 2 1 2 2 2\n"
      ],
      "metadata": {
        "id": "Mz3Dfwqk1KYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score,recall_score, f1_score\n",
        "actual = np.array([\n",
        " [0, 0, 1, 1, 2, 0],\n",
        " [0, 0, 1, 0, 2, 0],\n",
        " [0, 1, 1, 2, 2, 1],\n",
        " [0, 2, 1, 0, 2, 2],\n",
        " [0, 2, 1, 2, 2, 2]\n",
        "])\n",
        "predicted = np.array([\n",
        " [0, 0, 1, 1, 2, 0],\n",
        " [0, 0, 1, 0, 2, 0],\n",
        " [0, 1, 1, 2, 2, 1],\n",
        " [0, 2, 1, 0, 2, 2],\n",
        " [0, 2, 1, 2, 2, 2]\n",
        "])\n",
        "actual_flat = actual.flatten()\n",
        "predicted_flat = predicted.flatten()\n",
        "accuracy_manual = np.sum(actual_flat == predicted_flat) /len(actual_flat)\n",
        "unique_classes = np.unique(actual_flat)\n",
        "precision_manual = []\n",
        "recall_manual = []\n",
        "f1_manual = []\n",
        "for cls in unique_classes:\n",
        " tp = np.sum((actual_flat == cls) & (predicted_flat == cls))\n",
        " fp = np.sum((actual_flat != cls) & (predicted_flat == cls))\n",
        " fn = np.sum((actual_flat == cls) & (predicted_flat != cls))\n",
        " precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        " recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        " f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        " precision_manual.append(precision)\n",
        " recall_manual.append(recall)\n",
        " f1_manual.append(f1)\n",
        "accuracy_sklearn = accuracy_score(actual_flat, predicted_flat)\n",
        "precision_sklearn = precision_score(actual_flat, predicted_flat,\n",
        "average=None)\n",
        "recall_sklearn = recall_score(actual_flat, predicted_flat,\n",
        "average=None)\n",
        "f1_sklearn = f1_score(actual_flat, predicted_flat, average=None)\n",
        "print(\"Manual Calculations:\")\n",
        "print(f\"Accuracy: {accuracy_manual}\")\n",
        "print(f\"Precision: {precision_manual}\")\n",
        "print(f\"Recall: {recall_manual}\")\n",
        "print(f\"F1-Score: {f1_manual}\")\n",
        "print(\"\\nUsing sklearn:\")\n",
        "print(f\"Accuracy: {accuracy_sklearn}\")\n",
        "print(f\"Precision: {precision_sklearn}\")\n",
        "print(f\"Recall: {recall_sklearn}\")\n",
        "print(f\"F1-Score: {f1_sklearn}\")\n",
        "if (np.isclose(accuracy_manual, accuracy_sklearn) and\n",
        " np.allclose(precision_manual, precision_sklearn) and\n",
        " np.allclose(recall_manual, recall_sklearn) and\n",
        " np.allclose(f1_manual, f1_sklearn)):\n",
        " print(\"\\nManual calculations match the library results!\")\n",
        "else:\n",
        " print(\"\\nThere is a discrepancy between manual calculations and library results.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yeg3wx3w2V8F",
        "outputId": "bbbaadcd-fab6-4807-f4e3-a5fda5bd976e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manual Calculations:\n",
            "Accuracy: 1.0\n",
            "Precision: [1.0, 1.0, 1.0]\n",
            "Recall: [1.0, 1.0, 1.0]\n",
            "F1-Score: [1.0, 1.0, 1.0]\n",
            "\n",
            "Using sklearn:\n",
            "Accuracy: 1.0\n",
            "Precision: [1. 1. 1.]\n",
            "Recall: [1. 1. 1.]\n",
            "F1-Score: [1. 1. 1.]\n",
            "\n",
            "Manual calculations match the library results!\n"
          ]
        }
      ]
    }
  ]
}